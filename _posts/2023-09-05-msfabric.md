---
title: Medallion Lakehouse Architecture with Microsoft Fabric
date: 2023-09-05 12:00 -500
categories: [Architecture,Azure,Fabric,Data]
tags: [medallion, lakehouse, azure, microsoft-fabric]
author: gowda
---
## Introduction 

This blog outlines steps involved in building a medallion lakehouse architecture in a microsoft fabric lakehouse using notebooks.

    * Create a Workspace
    * Create a Lakehouse and upload data to the Bronze layer
    * Transform the data and load it to the Silver Delta table
    * Transform the data further and load it to the Gold Delta tables
    * Explore the dataset and create relationship in the form of Fact table and Dimension tables 

## Architecture

![Desktop View](/assets/img/msfabric-medallion-arch.png)

## Create a Workspace

Login to MS Fabric and create new workspace ==> ... ==> Workspace settings ==> enable "Data model settings"
![Desktop View](/assets/img/workspace.png)

## Create a Lakehouse and upload raw data (Bronze layer)

- Switch to the Data engineering experience in the Fabric portal and create a data lakehouse (Sales) for the data you're going to analyze.
- Download the sample data form <https://github.com/MicrosoftLearning/dp-data/blob/main/orders.zip> and create Bronze layer as shown and upload the data.
![Desktop View](/assets/img/lakehouse.png)

## Transform data for intermediate state (Silver layer)

Let us use a notebook to transform the data and load it to a delta table in the silver layer. 

- On the Home page while viewing the contents of the bronze folder in your data lake, in the Open notebook menu, select New notebook. Name it "Transform data for Silver".
- Execute the following code in each cell
    ``` python 

        from pyspark.sql.types import *

    # Create the schema for the table
    orderSchema = StructType([
        StructField("SalesOrderNumber", StringType()),
        StructField("SalesOrderLineNumber", IntegerType()),
        StructField("OrderDate", DateType()),
        StructField("CustomerName", StringType()),
        StructField("Email", StringType()),
        StructField("Item", StringType()),
        StructField("Quantity", IntegerType()),
        StructField("UnitPrice", FloatType()),
        StructField("Tax", FloatType())
        ])

    # Import all files from bronze folder of lakehouse
    df = spark.read.format("csv").option("header", "true").schema(orderSchema).load("Files/bronze/*.csv")

    # Display the first 10 rows of the dataframe to preview your data
    display(df.head(10))
    ```
    The code you ran loaded the data from the CSV files in the bronze folder into a Spark dataframe, and then displayed the first few rows of the dataframe.

- Now you'll add columns for data validation and cleanup, using a PySpark dataframe to add columns and update the values of some of the existing columns.
    ``` python
        from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name

        # Add columns IsFlagged, CreatedTS and ModifiedTS
        df = df.withColumn("FileName", input_file_name()) \
            .withColumn("IsFlagged", when(col("OrderDate") < '2019-08-01',True).otherwise(False)) \
            .withColumn("CreatedTS", current_timestamp()).withColumn("ModifiedTS", current_timestamp())

        # Update CustomerName to "Unknown" if CustomerName null or empty
        df = df.withColumn("CustomerName", when((col("CustomerName").isNull() | (col("CustomerName")=="")),lit("Unknown")).otherwise(col("CustomerName")))
    ```
- Next, you'll define the schema for the sales_silver table in the sales database using Delta Lake format.
    ``` python
        # Define the schema for the sales_silver table

        from pyspark.sql.types import *
        from delta.tables import *

        DeltaTable.createIfNotExists(spark) \
            .tableName("sales.sales_silver") \
            .addColumn("SalesOrderNumber", StringType()) \
            .addColumn("SalesOrderLineNumber", IntegerType()) \
            .addColumn("OrderDate", DateType()) \
            .addColumn("CustomerName", StringType()) \
            .addColumn("Email", StringType()) \
            .addColumn("Item", StringType()) \
            .addColumn("Quantity", IntegerType()) \
            .addColumn("UnitPrice", FloatType()) \
            .addColumn("Tax", FloatType()) \
            .addColumn("FileName", StringType()) \
            .addColumn("IsFlagged", BooleanType()) \
            .addColumn("CreatedTS", DateType()) \
            .addColumn("ModifiedTS", DateType()) \
            .execute()
    ```
- Now you're going to perform an upsert operation on a Delta table, updating existing records based on specific conditions and inserting new records when no match is found.

    This operation is important because it enables you to update existing records in the table based on the values of specific columns, and insert new records when no match is found. This is a common requirement when you're loading data from a source system that may contain updates to existing and new records.
    ``` python
        # Update existing records and insert new ones based on a condition defined by the columns SalesOrderNumber, OrderDate, CustomerName, and Item.

    from delta.tables import *

    deltaTable = DeltaTable.forPath(spark, 'Tables/sales_silver')

    dfUpdates = df

    deltaTable.alias('silver') \
    .merge(
        dfUpdates.alias('updates'),
        'silver.SalesOrderNumber = updates.SalesOrderNumber and silver.OrderDate = updates.OrderDate and silver.CustomerName = updates.CustomerName and silver.Item = updates.Item'
    ) \
    .whenMatchedUpdate(set =
        {
        
        }
    ) \
    .whenNotMatchedInsert(values =
        {
        "SalesOrderNumber": "updates.SalesOrderNumber",
        "SalesOrderLineNumber": "updates.SalesOrderLineNumber",
        "OrderDate": "updates.OrderDate",
        "CustomerName": "updates.CustomerName",
        "Email": "updates.Email",
        "Item": "updates.Item",
        "Quantity": "updates.Quantity",
        "UnitPrice": "updates.UnitPrice",
        "Tax": "updates.Tax",
        "FileName": "updates.FileName",
        "IsFlagged": "updates.IsFlagged",
        "CreatedTS": "updates.CreatedTS",
        "ModifiedTS": "updates.ModifiedTS"
        }
    ) \
    .execute()
    ```
    You now have data in your silver delta table that is ready for further transformation and modeling.

## Explore Silver layer with Sql endpoints

Now that you have data in your silver layer, you can use the SQL endpoint to explore the data and perform some basic analysis. This is a nice option for you if you're familiar with SQL and want to do some basic exploration of your data. In this exercise we're using the SQL endpoint view in Fabric, but note that you can also use other tools like SQL Server Management Studio (SSMS) and Azure Data Explorer.
![Desktop View](/assets/img/sqlendpoint.png)
![Desktop View](/assets/img/sqlquery.png)

## Transform data for final state, model into "Star schema" and Load into Gold layer Delta tables.
You have successfully taken data from your bronze layer, transformed it, and loaded it into a silver Delta table. Now you'll use a new notebook to transform the data further, model it into a star schema, and load it into gold Delta tables.

## References:
* <https://piethein.medium.com/using-dbt-for-building-a-medallion-lakehouse-architecture-azure-databricks-delta-dbt-31a31fc9bd0>
* <https://www.youtube.com/watch?v=KsO2FHQdILs>
* <https://anujsen02.medium.com/analytics-engineering-on-the-lakehouse-using-dbt-databricks-part-1-c4d773731ffe>
* <https://app.pluralsight.com/library/courses/building-etl-pipeline-microsoft-azure-databricks/table-of-contents>
* <https://www.youtube.com/watch?v=x3-qUw9XWMA>
